{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyON57hBwPolNNYxYJlZz5wG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alex112525/LangGraph-notebooks/blob/main/Parallelization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langgraph langgraph-sdk langgraph-checkpoint-sqlite langsmith langchain-community langchain-core langchain-openai tavily-python wikipedia"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6_f_UXjQt_RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RKG1FQB1sh5D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import operator\n",
        "from datetime import datetime\n",
        "from typing import Any, Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from google.colab import userdata\n",
        "from IPython.display import Image, display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.checkpoint.memory import MemorySaver"
      ],
      "metadata": {
        "id": "7Q2fIQPu3oac"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_community.retrievers import TavilySearchAPIRetriever"
      ],
      "metadata": {
        "id": "oyt1sJWewAXL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get('Azure_openai')\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get('Endpoint_openai')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('Tavily_API_key')"
      ],
      "metadata": {
        "id": "ma4zeB2TwaDl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils Funtions"
      ],
      "metadata": {
        "id": "I3YSHIzQvyN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model:str=\"DASH-mini\", temp:float=0.1, max_tokens:int=100):\n",
        "  \"\"\"Get model from Azure OpenAI\"\"\"\n",
        "  model = AzureChatOpenAI(\n",
        "        openai_api_version=\"2024-02-15-preview\",\n",
        "        azure_deployment=model,\n",
        "        temperature=temp,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "  return model"
      ],
      "metadata": {
        "id": "Y-jdvuBbvxpH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4o_mini = get_model(max_tokens= 500)"
      ],
      "metadata": {
        "id": "u3XW_8nHwzB9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "  # Append from Sources\n",
        "  question: str\n",
        "  answer: str\n",
        "  context: Annotated[list, operator.add]"
      ],
      "metadata": {
        "id": "Cr0vrhFr2NjR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools to search"
      ],
      "metadata": {
        "id": "1YLkLKYS7YV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def web_search(state):\n",
        "  \"\"\"Retrieve docs from web search\"\"\"\n",
        "\n",
        "  # Search\n",
        "  print(datetime.now(), \"- Start Web search\")\n",
        "  tavily_search = TavilySearchResults(max_results = 3)\n",
        "  search_docs = tavily_search.invoke(state[\"question\"])\n",
        "\n",
        "  # Format\n",
        "  formatted_search_docs = \"\\n---\\n\".join(\n",
        "      [f'<Document href={doc[\"url\"]}/>\\n{doc[\"content\"]}\\n </Document>' for doc in search_docs]\n",
        "  )\n",
        "  print(datetime.now(), \"- End Web search\")\n",
        "  return {\"context\": [formatted_search_docs]}"
      ],
      "metadata": {
        "id": "edDblQiP2Nao"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wiki_search(state):\n",
        "  \"\"\"Retrieve docs from wikipedia\"\"\"\n",
        "\n",
        "  # Search\n",
        "  print(datetime.now(), \"- Start Wiki search\")\n",
        "  search_docs = WikipediaLoader(query=state[\"question\"], load_max_docs=2).load()\n",
        "\n",
        "  # Format\n",
        "  formatted_search_docs = \"\\n---\\n\".join(\n",
        "      [f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>' for doc in search_docs]\n",
        "  )\n",
        "  print(datetime.now(), \"- End Wikipedia search\")\n",
        "  return {\"context\": [formatted_search_docs]}"
      ],
      "metadata": {
        "id": "csxTnAo42NM1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(state):\n",
        "  \"\"\"Node to answer a question\"\"\"\n",
        "\n",
        "  # Get state\n",
        "  context = state[\"context\"]\n",
        "  question = state[\"question\"]\n",
        "\n",
        "  # Template\n",
        "  answer_template = \"\"\"Answer the question {question}, using the following context {context}\"\"\"\n",
        "  answer_instructions = answer_template.format(question=question, context=context)\n",
        "\n",
        "  # Answer\n",
        "  ans = gpt4o_mini.invoke([SystemMessage(content=answer_instructions)])\n",
        "\n",
        "  return {\"answer\": ans}"
      ],
      "metadata": {
        "id": "iB4e8A5a6DzK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Graph"
      ],
      "metadata": {
        "id": "nvFMWoxn7ftm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add nodes\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"Web search\", web_search)\n",
        "builder.add_node(\"Wiki search\", wiki_search)\n",
        "builder.add_node(\"Generate answer\", generate_answer)\n",
        "\n",
        "# Flow\n",
        "builder.add_edge(START, \"Wiki search\")\n",
        "builder.add_edge(START, \"Web search\")\n",
        "builder.add_edge(\"Web search\", \"Generate answer\")\n",
        "builder.add_edge(\"Wiki search\", \"Generate answer\")\n",
        "builder.add_edge(\"Generate answer\", END)\n",
        "\n",
        "graph = builder.compile()\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "vehJaN-V7hkR",
        "outputId": "afc96fe2-57c9-4121-d4c7-e762c417258d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = graph.invoke({\"question\": \"What is Attention in human cognition\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkLQ-Rv57hpm",
        "outputId": "4c6221de-bc9f-45b3-a4fc-2bf2e3b03453"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-03 01:34:44.699734 - Start Web search\n",
            "2024-10-03 01:34:44.706278 - Start Wiki search\n",
            "2024-10-03 01:34:46.978555 - End Wikipedia search\n",
            "2024-10-03 01:34:47.546697 - End Web search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the time, we can see that both searches were running in parallel at the same time."
      ],
      "metadata": {
        "id": "OwYAJsYNAPZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results[\"context\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT9_4ZeR-QtJ",
        "outputId": "69054d71-c36d-498e-a875-72a360ae6a5b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<Document href=https://practicalpie.com/attention-psychology//>\\nIt underscores a fundamental aspect of human cognition: our attention is limited, even when focused intently on a task. It reveals the boundaries of human perception and illustrates how easy it is for us to miss even glaringly obvious events when our attention is directed elsewhere. This has broader implications in driving safety, aviation, and ...\\n </Document>\\n---\\n<Document href=https://www.verywellmind.com/what-is-attention-2795009/>\\nAttention allows you to \"tune out\" information, sensations, and perceptions that are not relevant at the moment and instead focus your energy on the information that\\'s important.\\u200b\\nNot only does our attentional system allow us to focus on something specific in our environment while tuning out irrelevant details, but it also affects our perception of the stimuli surrounding us.\\n In his 1890 book “The Principles of Psychology,” psychologist and philosopher William James wrote that attention \"is the taking possession by the mind, in clear and vivid form, of one out of what may seem several simultaneously possible objects or trains of thought…It implies withdrawal from some things in order to deal effectively with others. Key variables that impact our ability to stay on task include how interested we are in the stimulus and how many distractions there are.\\nTypes of Attention\\nThere are many different types of attention that people may use. Attention is limited in terms of both capacity and duration, so it is important to have ways to effectively manage the attentional resources we have available in order to make sense of the world.\\n This type of attention requires you to be able to tune out extraneous external stimuli, but also internal distractions such as thoughts and emotions in order to stay selectively attuned to a task.\\n\\n </Document>\\n---\\n<Document href=https://journals.sagepub.com/doi/full/10.1177/0963721420969371/>\\nOur interest in attention control stems from the close relationship between working memory—the cognitive system responsible for the temporary maintenance of information in a highly accessible state (Baddeley & Hitch, 1974)—and fluid intelligence—the ability to solve novel problems and adapt to new situations (Engle, Tuholski, Laughlin, & Conway, 1999).\\n </Document>',\n",
              " '<Document source=\"https://en.wikipedia.org/wiki/Michael_Tomasello\" page=\"\"/>\\nMichael Tomasello (born January 18, 1950) is an American developmental and comparative psychologist, as well as a linguist. He is professor of psychology at Duke University.\\nEarning many prizes and awards from the end of the 1990s onward, he is considered one of today\\'s most authoritative developmental and comparative psychologists. He is \"one of the few scientists worldwide who is acknowledged as an expert in multiple disciplines\". His \"pioneering research on the origins of social cognition has led to revolutionary insights in both developmental psychology and primate cognition.\"\\n\\n\\n== Early life and education ==\\nTomasello was born in Bartow, Florida and attended high school at the Taft School in Watertown, Connecticut. He received his bachelor\\'s degree 1972 from Duke University and his doctorate in Experimental Psychology 1980 from University of Georgia.\\n\\n\\n== Career ==\\nTomasello was a professor of psychology and anthropology at Emory University in Atlanta, Georgia, US, during the 1980s and 1990s. Subsequently, he moved to Germany to become co-director of Max Planck Institute for Evolutionary Anthropology in Leipzig, and later also honorary professor at University of Leipzig and co-director of the Wolfgang Kohler Primate Research Center. In 2016, he became professor of psychology and neuroscience at Duke University, where he now is James F. Bonk Distinguished Professor.\\nHe works on child language acquisition as a crucially important aspect of the enculturation process. He is a critic of Noam Chomsky\\'s universal grammar, rejecting the idea of an innate universal grammar and instead proposing a functional theory of language development (sometimes called the social-pragmatic theory of language acquisition or usage-based approach to language acquisition) in which children learn linguistic structures through intention-reading and pattern-finding in their discourse interactions with others.\\nTomasello also studies broader cognitive skills in a comparative light at the Wolfgang Köhler Primate Research Center in Leipzig. With his research team, he created a set of experimental devices to test toddlers\\' (from 6 months to 24 months) and apes\\' spatial, instrumental, and social cognition; the outcome of which is that social (even ultrasocial) cognition is what truly sets human apart.\\n\\n\\n=== Uniqueness of human social cognition: broad outlines ===\\nMore specifically, Tomasello argues that non-human apes lack a series of skills [ see https://www.pnas.org/doi/10.1073/pnas.1812244115 for more detail]:\\n\\nsocial learning through pedagogical ostension and deliberate transmission;\\nover-imitation, imitating not only action but also manners and styles of doing;\\ninformative pointing;\\nperspectival views, looking at the same thing or event alternatively from another agent\\'s angle;\\nrecursive mind reading, knowing what others know we know they know (and so forth);\\nthird-party punishment (when agent C punishes or avoids collaborating with agent B because of agent B\\'s unfairness toward agent A);\\nbuilding and enlarging common ground (communicating in order to share with others, and building a sphere of things that are commonly known);\\ngroup-mindedness (prescriptive feeling of belonging, of interdependence, of self-monitoring following general, impersonal expectations); and\\ncumulative culture, sometimes coined \"the ratchet effect\".\\nTomasello sees these skills as being preceded and encompassed by the capacity to share attention and intention (collective intentionality), an evolutionary novelty that would have emerged as a cooperative integrating of apes skills that formerly worked in competition.\\n\\n\\n=== The sharing of attention and of intention ===\\nThe overall scheme of sharing of attention and of intention involves inferring a common need; being motivated to act cooperatively to fulfill this need; coordinating individuals\\' roles and perspectives under the common goal of fulfilling this common need if, and only if, other agents fulfill their commitment toward tha\\n</Document>\\n---\\n<Document source=\"https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\" page=\"\"/>\\n\"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal Generative AI.\\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Uszkoreit liked the sound of that word.\\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\\nAs of 2024, the paper has been cited more than 100,000 times.\\nFor their 100M-parameter Transformer model, they suggested learning rate should be linearly scaled up from 0 to maximal value for the first part of the training (i.e. 2% of the total number of training steps), and to use dropout, to stabilize training.\\n\\n\\n== Authors ==\\nThe authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized. The Wired article highlights the group\\'s diversity:\\n\\nSix of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.\\n\\nBy 2023, all eight authors had left Google and founded their own AI start-ups (except Łukasz Kaiser, who joined OpenAI).\\n\\n\\n== Historical context ==\\n\\n\\n=== Predecessors ===\\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model\\'s state at the end of a long sentence without precise, extractable information about preceding tokens.\\nA key breakthrough was LSTM (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. Neural networks using multiplicative units were later called sigma-pi networks or higher-order networks. LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\\nHowever, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. \\nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window.  The linearly scaling fast weight controller (1992) learns  to compute a  weight matrix for further processing depending on the input. One of its two networks has  \"fast weights\" or \"dynamic links\" (1981). A slow neural networ\\n</Document>']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(results[\"answer\"].content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "b838LQj1-Eyu",
        "outputId": "f9dece17-2f75-496b-9ef7-79a7b7e6150a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Attention in human cognition refers to the mental process that allows individuals to focus on specific stimuli or tasks while filtering out irrelevant information. This cognitive function is crucial for managing the limited capacity of our attentional resources, enabling us to concentrate on what is most important at any given moment. \n\nAs highlighted in the provided context, attention is inherently limited, meaning that even when we are intensely focused, we can easily miss significant events occurring around us. This limitation underscores the boundaries of human perception and has practical implications in various fields, such as driving and aviation.\n\nWilliam James, a prominent psychologist, described attention as the \"taking possession by the mind\" of one out of several possible objects or thoughts, emphasizing the need to withdraw from some stimuli to effectively engage with others. The ability to manage attention is influenced by factors such as interest in the stimulus and the presence of distractions.\n\nThere are different types of attention, each serving distinct functions in our cognitive processes. Effective attention management is essential for making sense of the world, as it allows us to tune out extraneous stimuli, both external and internal, and remain focused on our tasks."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = graph.invoke({\"question\": \"What is a Transformer and why is important in AI\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqcJHG3A_JwW",
        "outputId": "2e5a65d3-8533-4fa0-d8ac-9c9f096b7f2b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-10-03 01:34:49.408271 - Start Web search\n",
            "2024-10-03 01:34:49.417260 - Start Wiki search\n",
            "2024-10-03 01:34:51.760849 - End Wikipedia search\n",
            "2024-10-03 01:34:51.901916 - End Web search\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(results[\"answer\"].content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "aDvpvfGD_ZYD",
        "outputId": "3ab30272-947b-47f3-986d-2173910dd12c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A **Transformer** is a type of neural network architecture that has revolutionized the field of artificial intelligence (AI), particularly in natural language processing (NLP). Introduced in a 2017 paper titled \"Attention is All You Need,\" the Transformer architecture is designed to handle sequential data, such as text, by using a mechanism called **self-attention**. This allows the model to weigh the importance of different words in a sentence relative to each other, capturing context and relationships more effectively than previous models like recurrent neural networks (RNNs) or convolutional neural networks (CNNs).\n\n### Importance in AI:\n\n1. **Contextual Understanding**: Transformers excel at understanding the interplay of words in a sentence, which is crucial for tasks like translation, summarization, and sentiment analysis. Models like BERT and GPT, which are based on the Transformer architecture, have set new benchmarks in these areas.\n\n2. **Scalability**: Transformers can be scaled up to billions of parameters, enabling them to capture a wide range of human language and knowledge. This scalability has led to the development of large language models that can perform various tasks with minimal fine-tuning.\n\n3. **Transfer Learning**: The architecture supports techniques like transfer learning, allowing models to be pre-trained on large datasets and then fine-tuned for specific tasks. This makes it easier and faster to customize models for different applications.\n\n4. **Generative Capabilities**: Transformers are foundational to generative AI, enabling the creation of coherent and contextually relevant text, images, and other media. This has led to significant advancements in creative tools and applications.\n\n5. **Broader Applications**: Beyond language, Transformers are being applied in fields like genomics, where they can analyze DNA sequences similarly to language, predicting the effects of genetic mutations and identifying disease-related patterns.\n\nIn summary, the Transformer architecture is crucial in AI because it enhances the ability of machines to understand and generate human-like text, supports large-scale learning, and opens up new avenues for innovation across various domains."
          },
          "metadata": {}
        }
      ]
    }
  ]
}